{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Qwen3-VL-30B-A3B-Instruct 推理流程详解\n",
                "\n",
                "本 Notebook 详细讲解 Qwen3-VL-30B-A3B-Instruct 模型的完整推理流程，包括：\n",
                "- **Vision Encoding**: 图像/视频的视觉编码\n",
                "- **Prefill**: 首次处理完整输入序列\n",
                "- **Decode**: 逐 token 自回归生成\n",
                "- **MoE (Mixture of Experts)**: 稀疏专家路由机制"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 模型架构总览\n",
                "\n",
                "```\n",
                "┌─────────────────────────────────────────────────────────────────┐\n",
                "│                    Qwen3-VL-30B-A3B-Instruct                    │\n",
                "├─────────────────────────────────────────────────────────────────┤\n",
                "│  ┌─────────────────┐     ┌──────────────────────────────────┐  │\n",
                "│  │  Vision Encoder │     │      Language Model (MoE)        │  │\n",
                "│  │  ───────────────│     │  ────────────────────────────────│  │\n",
                "│  │  • Patch Embed  │────▶│  • Token Embedding               │  │\n",
                "│  │  • ViT Blocks   │     │  • Decoder Layers ×N             │  │\n",
                "│  │  • Patch Merger │     │    - Self-Attention (mRoPE)      │  │\n",
                "│  └─────────────────┘     │    - MoE FFN (128 experts, top8) │  │\n",
                "│                          │  • LM Head                       │  │\n",
                "│                          └──────────────────────────────────┘  │\n",
                "└─────────────────────────────────────────────────────────────────┘\n",
                "```\n",
                "\n",
                "**关键参数:**\n",
                "- 总参数: ~30B\n",
                "- 激活参数: ~3B (每 token 只激活 8 个专家)\n",
                "- 专家数量: 128 个 routed experts\n",
                "- Top-K: 8"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. 完整推理流程"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 伪代码: Qwen3-VL-MoE 完整推理流程\n",
                "# ============================================================\n",
                "\n",
                "class Qwen3VLMoeForConditionalGeneration:\n",
                "    \"\"\"Qwen3-VL-MoE 模型主类\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        # 视觉编码器\n",
                "        self.visual = Qwen3_VisionTransformer()\n",
                "        # 语言模型 (MoE 架构)\n",
                "        self.language_model = Qwen3MoeLLMForCausalLM()\n",
                "    \n",
                "    def forward(self, input_ids, positions, pixel_values=None, image_grid_thw=None):\n",
                "        \"\"\"\n",
                "        主前向传播函数\n",
                "        \n",
                "        Args:\n",
                "            input_ids: [batch_size, seq_len] 文本 token IDs\n",
                "            positions: [3, seq_len] 或 [seq_len] 位置编码 (mRoPE)\n",
                "            pixel_values: [N, C, H, W] 图像像素值\n",
                "            image_grid_thw: [N, 3] 图像的 (T, H, W) 网格\n",
                "        \"\"\"\n",
                "        # ========== 阶段1: 视觉编码 ==========\n",
                "        if pixel_values is not None:\n",
                "            # 对图像进行编码\n",
                "            vision_embeddings = self.visual(pixel_values, image_grid_thw)\n",
                "            # shape: [num_patches, hidden_size]\n",
                "            \n",
                "            # 将视觉 embedding 合并到文本 embedding 中\n",
                "            inputs_embeds = self._merge_multimodal_embeddings(\n",
                "                input_ids, vision_embeddings\n",
                "            )\n",
                "        else:\n",
                "            inputs_embeds = self.language_model.embed_tokens(input_ids)\n",
                "        \n",
                "        # ========== 阶段2: LLM 前向传播 (包含 MoE) ==========\n",
                "        hidden_states = self.language_model.model(\n",
                "            input_ids=None,\n",
                "            positions=positions,\n",
                "            inputs_embeds=inputs_embeds,\n",
                "        )\n",
                "        \n",
                "        # ========== 阶段3: 输出预测 ==========\n",
                "        logits = self.language_model.lm_head(hidden_states)\n",
                "        return logits"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Vision Encoder 详解"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Qwen3_VisionTransformer:\n",
                "    \"\"\"视觉编码器 - 将图像转换为 embedding\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.patch_embed = PatchEmbed3D()      # 3D 卷积 patch embedding\n",
                "        self.blocks = [VisionBlock() for _ in range(depth)]  # ViT blocks\n",
                "        self.merger = PatchMerger()            # 空间下采样\n",
                "    \n",
                "    def forward(self, x, grid_thw):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            x: [N, C, T, H, W] 输入图像/视频\n",
                "               - N: batch 中的图像数量\n",
                "               - C: 通道数 (3 for RGB)\n",
                "               - T: 时间维度 (图像=1, 视频>1)\n",
                "               - H, W: 高度和宽度\n",
                "            grid_thw: [N, 3] 每个图像的 (T, H, W) 网格尺寸\n",
                "        \n",
                "        Returns:\n",
                "            hidden_states: [total_patches, hidden_size]\n",
                "        \"\"\"\n",
                "        # Step 1: Patch Embedding (3D卷积)\n",
                "        # 将图像分割成 14x14 的 patches\n",
                "        hidden_states = self.patch_embed(x)\n",
                "        # shape: [total_patches, hidden_size]\n",
                "        # 例如 448x448 图像 -> (448/14)^2 = 1024 patches\n",
                "        \n",
                "        # Step 2: 添加位置编码 (2D RoPE)\n",
                "        pos_embeds = self.compute_position_embedding(grid_thw)\n",
                "        hidden_states = hidden_states + pos_embeds\n",
                "        \n",
                "        # Step 3: Vision Transformer Blocks\n",
                "        for block in self.blocks:\n",
                "            hidden_states = block(\n",
                "                hidden_states,\n",
                "                rotary_pos_emb=self.rope_embed,\n",
                "            )\n",
                "        \n",
                "        # Step 4: Patch Merger (空间下采样 2x2)\n",
                "        # 将相邻的 2x2 patches 合并为 1 个\n",
                "        hidden_states = self.merger(hidden_states)\n",
                "        # patches 数量减少 4 倍: 1024 -> 256\n",
                "        \n",
                "        return hidden_states\n",
                "\n",
                "# Vision 输出示例:\n",
                "# 输入: 448x448 RGB 图像\n",
                "# Patch Embed 后: [1024, 1280] (1024 patches, 1280-dim)\n",
                "# Merger 后: [256, 3584] (256 tokens, 3584-dim 匹配 LLM hidden_size)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. MoE Layer 详解"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Qwen3MoeSparseMoeBlock:\n",
                "    \"\"\"\n",
                "    MoE (Mixture of Experts) 层\n",
                "    \n",
                "    核心思想: 不是所有 token 都经过所有参数\n",
                "    每个 token 只选择 top-K 个专家进行计算\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.num_experts = 128          # 总专家数\n",
                "        self.top_k = 8                  # 每 token 激活的专家数\n",
                "        self.gate = Linear(hidden_size, num_experts)  # 路由器\n",
                "        self.experts = FusedMoE(...)    # 128 个 FFN 专家\n",
                "    \n",
                "    def forward(self, hidden_states):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            hidden_states: [num_tokens, hidden_size]\n",
                "        \n",
                "        Returns:\n",
                "            output: [num_tokens, hidden_size]\n",
                "        \"\"\"\n",
                "        num_tokens, hidden_dim = hidden_states.shape\n",
                "        \n",
                "        # ========== Step 1: Router/Gate 计算 ==========\n",
                "        # 计算每个 token 对每个专家的偏好分数\n",
                "        router_logits = self.gate(hidden_states)\n",
                "        # shape: [num_tokens, num_experts=128]\n",
                "        \n",
                "        # ========== Step 2: Top-K 选择 ==========\n",
                "        # 对每个 token, 选择得分最高的 K 个专家\n",
                "        topk_weights, topk_ids = fused_topk(\n",
                "            hidden_states,\n",
                "            router_logits,\n",
                "            topk=8,\n",
                "            renormalize=True,  # 权重归一化\n",
                "        )\n",
                "        # topk_weights: [num_tokens, 8] 专家权重\n",
                "        # topk_ids: [num_tokens, 8] 专家 ID\n",
                "        \n",
                "        # ========== Step 3: Expert FFN 计算 ==========\n",
                "        # 使用 Fused MoE Kernel 高效计算\n",
                "        output = self.experts(\n",
                "            hidden_states=hidden_states,\n",
                "            router_logits=router_logits,\n",
                "        )\n",
                "        # 内部流程:\n",
                "        # 1. 按专家分组 tokens\n",
                "        # 2. 并行计算各专家的 FFN\n",
                "        # 3. 加权聚合结果\n",
                "        \n",
                "        return output  # [num_tokens, hidden_size]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 MoE 路由可视化\n",
                "\n",
                "```\n",
                "Token 1: \"Hello\"  ─── Router ───▶ Expert 3 (0.35), Expert 17 (0.25), ... (top-8)\n",
                "Token 2: \"World\"  ─── Router ───▶ Expert 5 (0.40), Expert 3 (0.20), ... (top-8)\n",
                "Token 3: [IMAGE]  ─── Router ───▶ Expert 42 (0.30), Expert 8 (0.28), ... (top-8)\n",
                "    │\n",
                "    ▼\n",
                "┌────────────────────────────────────────────────────────────────┐\n",
                "│                     Expert Pool (128 experts)                  │\n",
                "├────────────────────────────────────────────────────────────────┤\n",
                "│ Expert 0  │ Expert 1  │ ... │ Expert 42 │ ... │ Expert 127    │\n",
                "│ (idle)    │ (idle)    │     │ (active)  │     │ (idle)        │\n",
                "└────────────────────────────────────────────────────────────────┘\n",
                "    │\n",
                "    ▼\n",
                "加权聚合: output = Σ(weight_i × expert_i(hidden_states))\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Prefill vs Decode 阶段对比"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Prefill 阶段: 一次性处理完整输入\n",
                "# ============================================================\n",
                "\n",
                "def prefill_phase(model, prompt_tokens, image):\n",
                "    \"\"\"\n",
                "    Prefill: 处理用户输入的完整 prompt + 图像\n",
                "    \n",
                "    特点:\n",
                "    - 输入序列较长 (prompt + image patches)\n",
                "    - 计算密集型 (compute-bound)\n",
                "    - Self-attention 是完整的 causal attention\n",
                "    - 生成 KV Cache 供 decode 阶段使用\n",
                "    \"\"\"\n",
                "    # 输入示例\n",
                "    prompt = \"Describe this image in detail.\"\n",
                "    seq_len = len(prompt_tokens) + num_image_patches  # 例如: 50 + 256 = 306\n",
                "    \n",
                "    # Vision Encoding (只在 prefill 执行一次)\n",
                "    vision_embeds = model.visual(image, grid_thw)\n",
                "    # shape: [256, hidden_size]\n",
                "    \n",
                "    # 合并 text + vision embeddings\n",
                "    inputs_embeds = merge_embeddings(prompt_tokens, vision_embeds)\n",
                "    # shape: [306, hidden_size]\n",
                "    \n",
                "    # 通过所有 decoder layers (包含 MoE)\n",
                "    # Attention: [306, 306] 的 causal mask\n",
                "    hidden_states = model.forward(inputs_embeds, positions)\n",
                "    \n",
                "    # 生成第一个 token\n",
                "    first_token_logits = model.lm_head(hidden_states[-1])\n",
                "    first_token = sample(first_token_logits)\n",
                "    \n",
                "    # 保存 KV Cache\n",
                "    kv_cache = save_kv_cache()  # [num_layers, 2, seq_len, num_heads, head_dim]\n",
                "    \n",
                "    return first_token, kv_cache"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Decode 阶段: 逐 token 生成\n",
                "# ============================================================\n",
                "\n",
                "def decode_phase(model, kv_cache, last_token):\n",
                "    \"\"\"\n",
                "    Decode: 自回归生成, 每次只处理 1 个新 token\n",
                "    \n",
                "    特点:\n",
                "    - 输入序列长度 = 1 (单个新 token)\n",
                "    - 内存密集型 (memory-bound)\n",
                "    - 利用 KV Cache 避免重复计算\n",
                "    - Attention 只计算新 token 与所有历史 token\n",
                "    \"\"\"\n",
                "    # 输入: 上一步生成的 token\n",
                "    new_token_embed = model.embed_tokens(last_token)\n",
                "    # shape: [1, hidden_size]\n",
                "    \n",
                "    # Self-Attention 使用 KV Cache\n",
                "    # Query: [1, num_heads, head_dim] (只有新 token)\n",
                "    # Key:   [seq_len+1, num_heads, head_dim] (历史 + 新)\n",
                "    # Value: [seq_len+1, num_heads, head_dim]\n",
                "    \n",
                "    # MoE 计算 (仍然每个 token 选 top-8 专家)\n",
                "    # 但因为只有 1 个 token, 计算量大大减少\n",
                "    hidden_states = model.forward(\n",
                "        new_token_embed, \n",
                "        position=current_pos,\n",
                "        kv_cache=kv_cache,\n",
                "    )\n",
                "    \n",
                "    # 预测下一个 token\n",
                "    next_token_logits = model.lm_head(hidden_states)\n",
                "    next_token = sample(next_token_logits)\n",
                "    \n",
                "    # 更新 KV Cache\n",
                "    kv_cache = update_kv_cache(kv_cache, new_kv)\n",
                "    \n",
                "    return next_token, kv_cache\n",
                "\n",
                "# Decode 循环\n",
                "def generate(model, prompt, image, max_tokens=100):\n",
                "    # Prefill\n",
                "    token, kv_cache = prefill_phase(model, prompt, image)\n",
                "    generated = [token]\n",
                "    \n",
                "    # Decode loop\n",
                "    for _ in range(max_tokens):\n",
                "        token, kv_cache = decode_phase(model, kv_cache, token)\n",
                "        generated.append(token)\n",
                "        if token == EOS_TOKEN:\n",
                "            break\n",
                "    \n",
                "    return generated"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Prefill vs Decode 对比表\n",
                "\n",
                "| 特性 | Prefill | Decode |\n",
                "|------|---------|--------|\n",
                "| 输入长度 | 整个 prompt + 图像 patches | 1 个 token |\n",
                "| 计算特点 | Compute-bound | Memory-bound |\n",
                "| Attention | Full causal (NxN) | Query=1, KV=历史 |\n",
                "| KV Cache | 生成并保存 | 读取并更新 |\n",
                "| Vision Encoder | 执行 | 跳过 |\n",
                "| 执行次数 | 1 次 | 每个生成 token 1 次 |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Decoder Layer 完整流程"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Qwen3MoeDecoderLayer:\n",
                "    \"\"\"单个 Decoder Layer 的完整流程\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.input_layernorm = RMSNorm(hidden_size)\n",
                "        self.self_attn = Qwen3MoeAttention()  # 带 QK Norm 的注意力\n",
                "        self.post_attention_layernorm = RMSNorm(hidden_size)\n",
                "        self.mlp = Qwen3MoeSparseMoeBlock()  # MoE 层\n",
                "    \n",
                "    def forward(self, positions, hidden_states, residual):\n",
                "        \"\"\"\n",
                "        Pre-LN Transformer 架构 (LN 在子层之前)\n",
                "        \"\"\"\n",
                "        # ========== Self-Attention 部分 ==========\n",
                "        if residual is None:\n",
                "            residual = hidden_states\n",
                "            hidden_states = self.input_layernorm(hidden_states)\n",
                "        else:\n",
                "            hidden_states, residual = self.input_layernorm(\n",
                "                hidden_states, residual\n",
                "            )\n",
                "        \n",
                "        # Self-Attention with mRoPE (多分辨率旋转位置编码)\n",
                "        hidden_states = self.self_attn(\n",
                "            positions=positions,     # [3, seq_len] for mRoPE\n",
                "            hidden_states=hidden_states,\n",
                "        )\n",
                "        \n",
                "        # ========== MoE FFN 部分 ==========\n",
                "        hidden_states, residual = self.post_attention_layernorm(\n",
                "            hidden_states, residual\n",
                "        )\n",
                "        \n",
                "        # MoE: 稀疏专家计算\n",
                "        hidden_states = self.mlp(hidden_states)\n",
                "        # 每个 token 只激活 8/128 = 6.25% 的专家参数\n",
                "        \n",
                "        return hidden_states, residual"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. FusedMoE 内部实现"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def fused_moe_forward(hidden_states, router_logits, w1, w2, topk=8):\n",
                "    \"\"\"\n",
                "    Fused MoE 的核心计算流程 (简化版)\n",
                "    \n",
                "    实际实现使用 Triton kernel 进行融合优化\n",
                "    \"\"\"\n",
                "    num_tokens, hidden_dim = hidden_states.shape\n",
                "    num_experts = router_logits.shape[-1]  # 128\n",
                "    \n",
                "    # Step 1: TopK Routing\n",
                "    # 选择每个 token 的 top-8 专家\n",
                "    topk_weights = softmax(router_logits, dim=-1)  # 归一化\n",
                "    topk_weights, topk_ids = topk(topk_weights, k=8)\n",
                "    topk_weights = normalize(topk_weights)  # 重新归一化\n",
                "    \n",
                "    # Step 2: Token-to-Expert 分配\n",
                "    # 按专家分组 tokens (便于并行计算)\n",
                "    # sorted_token_ids: 按专家排序的 token 索引\n",
                "    # expert_ids: 对应的专家 ID\n",
                "    sorted_token_ids, expert_ids = moe_align_block_size(\n",
                "        topk_ids, block_size=128\n",
                "    )\n",
                "    \n",
                "    # Step 3: Expert FFN 计算 (Gate-Up-Down)\n",
                "    # 对每个专家:\n",
                "    #   gate = w1_gate @ hidden_states  # 门控\n",
                "    #   up = w1_up @ hidden_states       # 上投影\n",
                "    #   hidden = SiLU(gate) * up         # 激活\n",
                "    #   output = w2 @ hidden             # 下投影\n",
                "    \n",
                "    # Triton 融合内核: 一次 kernel launch 完成上述计算\n",
                "    expert_outputs = fused_moe_kernel(\n",
                "        A=hidden_states,           # 输入\n",
                "        B=w1,                       # 专家权重 [E, N, K]\n",
                "        sorted_token_ids=sorted_token_ids,\n",
                "        expert_ids=expert_ids,\n",
                "        topk_weights=topk_weights,\n",
                "    )\n",
                "    \n",
                "    # Step 4: 加权聚合\n",
                "    # output[i] = Σ_k (weight[i,k] × expert_output[i,k])\n",
                "    output = weighted_sum(expert_outputs, topk_weights)\n",
                "    \n",
                "    return output\n",
                "\n",
                "# 实际内存节省:\n",
                "# Dense FFN: 所有 token × 所有参数 = O(N × hidden × intermediate)\n",
                "# Sparse MoE: 所有 token × top-K 参数 = O(N × hidden × intermediate × K/E)\n",
                "# 节省比例: 8/128 = 6.25%"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. 总结: 完整数据流\n",
                "\n",
                "```\n",
                "输入: \"描述这张图片\" + [IMAGE]\n",
                "        │\n",
                "        ▼\n",
                "┌─────────────────────────────────────────────────────────────────┐\n",
                "│  Tokenizer: 文本 → Token IDs [1, 2, 3, 4, 5, <IMG>, 7, 8, ...]  │\n",
                "└─────────────────────────────────────────────────────────────────┘\n",
                "        │\n",
                "        ├────────────────────────────────────────┐\n",
                "        ▼                                        ▼\n",
                "┌─────────────────────┐              ┌─────────────────────┐\n",
                "│  Token Embedding    │              │   Vision Encoder    │\n",
                "│  [5, hidden_size]   │              │   [256, hidden_size]│\n",
                "└─────────────────────┘              └─────────────────────┘\n",
                "        │                                        │\n",
                "        └────────────────┬───────────────────────┘\n",
                "                         ▼\n",
                "           ┌───────────────────────────┐\n",
                "           │  Merge: [261, hidden_size] │ ◄── Prefill 输入\n",
                "           └───────────────────────────┘\n",
                "                         │\n",
                "         ┌───────────────┴───────────────┐\n",
                "         ▼                               ▼\n",
                "┌─────────────────┐           ┌─────────────────────────────┐\n",
                "│ Self-Attention  │           │         MoE Layer           │\n",
                "│ (with mRoPE)    │───────────▶  Router → Top-8 → Experts   │\n",
                "│ [261, hidden]   │           │  [261, hidden] → [261, hidden]│\n",
                "└─────────────────┘           └─────────────────────────────┘\n",
                "                                         │\n",
                "                              × N layers (循环)\n",
                "                                         │\n",
                "                                         ▼\n",
                "                         ┌───────────────────────┐\n",
                "                         │  LM Head → Logits     │\n",
                "                         │  [261, vocab_size]    │\n",
                "                         └───────────────────────┘\n",
                "                                         │\n",
                "                                         ▼\n",
                "                              取最后一个 token logits\n",
                "                                         │\n",
                "                                         ▼\n",
                "                         ┌───────────────────────┐\n",
                "                         │  Sampling → \"这\"       │ ◄── 第一个生成 token\n",
                "                         └───────────────────────┘\n",
                "                                         │\n",
                "                              Decode 循环...\n",
                "                                         │\n",
                "                                         ▼\n",
                "输出: \"这张图片展示了一只可爱的小猫...\"\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. 相关源码位置\n",
                "\n",
                "| 组件 | 文件路径 | 关键类/函数 |\n",
                "|------|---------|-------------|\n",
                "| 主模型 | `vllm/model_executor/models/qwen3_vl_moe.py` | `Qwen3VLMoeForConditionalGeneration` |\n",
                "| Vision Encoder | `vllm/model_executor/models/qwen3_vl.py` | `Qwen3_VisionTransformer` |\n",
                "| MoE Block | `vllm/model_executor/models/qwen3_moe.py` | `Qwen3MoeSparseMoeBlock` |\n",
                "| FusedMoE Layer | `vllm/model_executor/layers/fused_moe/layer.py` | `FusedMoE` |\n",
                "| MoE Kernels | `vllm/model_executor/layers/fused_moe/fused_moe.py` | `fused_topk`, `invoke_fused_moe_kernel` |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
#!/usr/bin/env python3
"""
vLLM Prefill å’Œ Decode é˜¶æ®µå®Œæ•´æµç¨‹æ¼”ç¤º

è¿™ä¸ªè„šæœ¬æ¨¡æ‹Ÿäº† LLM æ¨ç†çš„ä¸¤ä¸ªæ ¸å¿ƒé˜¶æ®µï¼š
1. Prefill é˜¶æ®µï¼šå¤„ç†å®Œæ•´çš„ promptï¼Œç”Ÿæˆ KV Cache
2. Decode é˜¶æ®µï¼šé€ token ç”Ÿæˆï¼Œä½¿ç”¨å·²ç¼“å­˜çš„ KV

ä»¥ Qwen3-4B æ¨¡å‹é…ç½®ä¸ºä¾‹ï¼Œå±•ç¤º TP=2 çš„å¹¶è¡Œè®¡ç®—
"""

import math
from dataclasses import dataclass
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F

# ============================================================================
# 1. æ¨¡å‹é…ç½® (åŸºäº Qwen3-4B)
# ============================================================================

@dataclass
class Qwen3Config:
    """Qwen3-4B æ¨¡å‹é…ç½®"""
    hidden_size: int = 2560
    num_attention_heads: int = 32
    num_key_value_heads: int = 8  # GQA: Grouped Query Attention
    head_dim: int = 128
    intermediate_size: int = 9728
    num_hidden_layers: int = 36
    vocab_size: int = 151936
    rms_norm_eps: float = 1e-6
    max_position_embeddings: int = 40960
    
    # Tensor Parallelism é…ç½®
    tp_size: int = 2  # é»˜è®¤ TP=2
    
    @property
    def num_heads_per_tp(self) -> int:
        """æ¯ä¸ª TP rank çš„ attention heads æ•°é‡"""
        return self.num_attention_heads // self.tp_size
    
    @property
    def num_kv_heads_per_tp(self) -> int:
        """æ¯ä¸ª TP rank çš„ KV heads æ•°é‡"""
        return self.num_key_value_heads // self.tp_size


# ============================================================================
# 2. KV Cache ç®¡ç†å™¨ (Paged Attention é£æ ¼)
# ============================================================================

class PagedKVCache:
    """
    Paged KV Cache ç®¡ç†å™¨
    
    æ¨¡æ‹Ÿ vLLM çš„ Paged Attention:
    - KV Cache è¢«åˆ†æˆå›ºå®šå¤§å°çš„ blocks
    - æ¯ä¸ªè¯·æ±‚åŠ¨æ€åˆ†é… blocks
    """
    
    def __init__(
        self,
        num_blocks: int,
        block_size: int,
        num_kv_heads: int,
        head_dim: int,
        dtype: torch.dtype = torch.float16,
        device: str = "cuda",
    ):
        self.num_blocks = num_blocks
        self.block_size = block_size
        self.num_kv_heads = num_kv_heads
        self.head_dim = head_dim
        self.dtype = dtype
        self.device = device
        
        # é¢„åˆ†é… KV Cache å­˜å‚¨
        # å½¢çŠ¶: [num_blocks, num_kv_heads, block_size, head_dim]
        self.k_cache = torch.zeros(
            (num_blocks, num_kv_heads, block_size, head_dim),
            dtype=dtype, device=device
        )
        self.v_cache = torch.zeros(
            (num_blocks, num_kv_heads, block_size, head_dim),
            dtype=dtype, device=device
        )
        
        # Block åˆ†é…è¡¨
        self.free_blocks = list(range(num_blocks))
        self.block_tables: dict[str, list[int]] = {}  # request_id -> block_ids
        
    def allocate_blocks(self, request_id: str, num_tokens: int) -> list[int]:
        """ä¸ºè¯·æ±‚åˆ†é…æ‰€éœ€çš„ blocks"""
        num_blocks_needed = math.ceil(num_tokens / self.block_size)
        
        if len(self.free_blocks) < num_blocks_needed:
            raise RuntimeError("KV Cache ç©ºé—´ä¸è¶³!")
        
        allocated = []
        for _ in range(num_blocks_needed):
            block_id = self.free_blocks.pop(0)
            allocated.append(block_id)
        
        if request_id not in self.block_tables:
            self.block_tables[request_id] = []
        self.block_tables[request_id].extend(allocated)
        
        return allocated
    
    def write_kv(
        self,
        request_id: str,
        key: torch.Tensor,     # [seq_len, num_kv_heads, head_dim]
        value: torch.Tensor,   # [seq_len, num_kv_heads, head_dim]
        start_pos: int,
    ):
        """å°† K, V å†™å…¥ç¼“å­˜"""
        seq_len = key.shape[0]
        block_ids = self.block_tables[request_id]
        
        for i in range(seq_len):
            pos = start_pos + i
            block_idx = pos // self.block_size
            offset_in_block = pos % self.block_size
            
            block_id = block_ids[block_idx]
            self.k_cache[block_id, :, offset_in_block, :] = key[i]
            self.v_cache[block_id, :, offset_in_block, :] = value[i]
    
    def read_kv(
        self,
        request_id: str,
        seq_len: int,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """è¯»å–å®Œæ•´çš„ KV Cache"""
        block_ids = self.block_tables[request_id]
        
        k_out = []
        v_out = []
        
        for pos in range(seq_len):
            block_idx = pos // self.block_size
            offset_in_block = pos % self.block_size
            block_id = block_ids[block_idx]
            
            k_out.append(self.k_cache[block_id, :, offset_in_block, :])
            v_out.append(self.v_cache[block_id, :, offset_in_block, :])
        
        return (
            torch.stack(k_out, dim=0),  # [seq_len, num_kv_heads, head_dim]
            torch.stack(v_out, dim=0),  # [seq_len, num_kv_heads, head_dim]
        )
    
    def free(self, request_id: str):
        """é‡Šæ”¾è¯·æ±‚çš„ blocks"""
        if request_id in self.block_tables:
            self.free_blocks.extend(self.block_tables[request_id])
            del self.block_tables[request_id]


# ============================================================================
# 3. RoPE (Rotary Position Embedding)
# ============================================================================

def precompute_freqs_cis(
    dim: int,
    end: int,
    theta: float = 1000000.0,
    device: str = "cuda",
) -> torch.Tensor:
    """é¢„è®¡ç®— RoPE çš„é¢‘ç‡"""
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))
    t = torch.arange(end, device=device, dtype=torch.float32)
    freqs = torch.outer(t, freqs)
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64
    return freqs_cis


def apply_rotary_emb(
    xq: torch.Tensor,  # [seq_len, num_heads, head_dim]
    xk: torch.Tensor,  # [seq_len, num_kv_heads, head_dim]
    freqs_cis: torch.Tensor,
    positions: torch.Tensor,  # [seq_len]
) -> tuple[torch.Tensor, torch.Tensor]:
    """åº”ç”¨ RoPE"""
    # é€‰æ‹©å¯¹åº”ä½ç½®çš„é¢‘ç‡
    freqs = freqs_cis[positions]  # [seq_len, head_dim/2]
    
    # å°† tensor è½¬æ¢ä¸ºå¤æ•°å½¢å¼
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    
    # åº”ç”¨æ—‹è½¬
    freqs = freqs.unsqueeze(1)  # [seq_len, 1, head_dim/2]
    xq_out = torch.view_as_real(xq_ * freqs).flatten(-2)
    xk_out = torch.view_as_real(xk_ * freqs).flatten(-2)
    
    return xq_out.type_as(xq), xk_out.type_as(xk)


# ============================================================================
# 4. Attention å±‚ (æ”¯æŒ GQA + RoPE + QK Norm)
# ============================================================================

class Qwen3Attention(nn.Module):
    """
    Qwen3 é£æ ¼çš„ Attention å±‚
    
    ç‰¹ç‚¹:
    - Grouped Query Attention (GQA)
    - QK Norm (RMSNorm on Q and K)
    - Rotary Position Embedding (RoPE)
    - æ”¯æŒ Tensor Parallelism
    """
    
    def __init__(self, config: Qwen3Config, layer_idx: int, tp_rank: int = 0):
        super().__init__()
        
        self.config = config
        self.layer_idx = layer_idx
        self.tp_rank = tp_rank
        self.tp_size = config.tp_size
        
        # TP åˆ†ç‰‡åçš„å‚æ•°
        self.num_heads = config.num_heads_per_tp
        self.num_kv_heads = config.num_kv_heads_per_tp
        self.head_dim = config.head_dim
        self.hidden_size = config.hidden_size
        
        self.q_size = self.num_heads * self.head_dim
        self.kv_size = self.num_kv_heads * self.head_dim
        
        self.scaling = self.head_dim ** -0.5
        
        # QKV Projection (Column Parallel)
        # æ¯ä¸ª TP rank æŒæœ‰ Q, K, V çš„ä¸€éƒ¨åˆ†
        self.qkv_proj = nn.Linear(
            self.hidden_size,
            self.q_size + 2 * self.kv_size,
            bias=False,
        )
        
        # Output Projection (Row Parallel)
        self.o_proj = nn.Linear(
            self.q_size,  # æœ¬åœ° input
            self.hidden_size,  # å…¨å±€ output
            bias=False,
        )
        
        # QK Norm (Qwen3 ç‰¹æœ‰)
        self.q_norm = nn.RMSNorm(self.head_dim, eps=config.rms_norm_eps)
        self.k_norm = nn.RMSNorm(self.head_dim, eps=config.rms_norm_eps)
    
    def forward(
        self,
        hidden_states: torch.Tensor,  # [seq_len, hidden_size]
        positions: torch.Tensor,      # [seq_len]
        freqs_cis: torch.Tensor,
        kv_cache: Optional[PagedKVCache] = None,
        request_id: str = "req_0",
        cache_start_pos: int = 0,
        is_prefill: bool = True,
    ) -> torch.Tensor:
        """
        Args:
            hidden_states: è¾“å…¥éšè—çŠ¶æ€
            positions: ä½ç½®ç´¢å¼•
            freqs_cis: RoPE é¢‘ç‡
            kv_cache: KV Cache ç®¡ç†å™¨
            request_id: è¯·æ±‚ ID
            cache_start_pos: KV Cache å†™å…¥èµ·å§‹ä½ç½®
            is_prefill: æ˜¯å¦æ˜¯ prefill é˜¶æ®µ
        """
        seq_len = hidden_states.shape[0]
        
        # =========== Step 1: QKV Projection ===========
        qkv = self.qkv_proj(hidden_states)
        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
        
        # Reshape: [seq_len, num_heads/kv_heads, head_dim]
        q = q.view(seq_len, self.num_heads, self.head_dim)
        k = k.view(seq_len, self.num_kv_heads, self.head_dim)
        v = v.view(seq_len, self.num_kv_heads, self.head_dim)
        
        # =========== Step 2: QK Norm ===========
        q = self.q_norm(q)
        k = self.k_norm(k)
        
        # =========== Step 3: Apply RoPE ===========
        q, k = apply_rotary_emb(q, k, freqs_cis, positions)
        
        # =========== Step 4: KV Cache æ“ä½œ ===========
        if kv_cache is not None:
            # å†™å…¥ KV Cache
            kv_cache.write_kv(request_id, k, v, cache_start_pos)
            
            # è¯»å–å®Œæ•´çš„ KV (åŒ…æ‹¬å†å² tokens)
            total_seq_len = cache_start_pos + seq_len
            k_cache, v_cache = kv_cache.read_kv(request_id, total_seq_len)
        else:
            k_cache, v_cache = k, v
            total_seq_len = seq_len
        
        # =========== Step 5: Attention è®¡ç®— ===========
        if is_prefill:
            attn_output = self._prefill_attention(q, k_cache, v_cache, seq_len)
        else:
            attn_output = self._decode_attention(q, k_cache, v_cache)
        
        # =========== Step 6: Output Projection ===========
        output = self.o_proj(attn_output.reshape(seq_len, -1))
        
        return output
    
    def _prefill_attention(
        self,
        q: torch.Tensor,  # [seq_len, num_heads, head_dim]
        k: torch.Tensor,  # [total_seq_len, num_kv_heads, head_dim]
        v: torch.Tensor,  # [total_seq_len, num_kv_heads, head_dim]
        query_len: int,
    ) -> torch.Tensor:
        """
        Prefill é˜¶æ®µçš„ Attention è®¡ç®—
        
        ç‰¹ç‚¹:
        - æ‰€æœ‰ query tokens åŒæ—¶è®¡ç®—
        - ä½¿ç”¨ causal mask
        - å¯ä»¥åˆ©ç”¨ FlashAttention ç­‰ä¼˜åŒ– kernel
        """
        total_seq_len = k.shape[0]
        
        # GQA: æ‰©å±• K, V åˆ°ä¸ Q ç›¸åŒçš„ heads æ•°
        num_kv_groups = self.num_heads // self.num_kv_heads
        k = k.repeat_interleave(num_kv_groups, dim=1)  # [total_seq, num_heads, head_dim]
        v = v.repeat_interleave(num_kv_groups, dim=1)
        
        # åªå…³æ³¨æœ€å query_len ä¸ªä½ç½® (chunked prefill åœºæ™¯)
        q_start = total_seq_len - query_len
        
        # è®¡ç®— attention scores
        # Q: [query_len, num_heads, head_dim]
        # K: [total_seq_len, num_heads, head_dim]
        scores = torch.einsum("qhd,khd->hqk", q, k) * self.scaling
        # scores: [num_heads, query_len, total_seq_len]
        
        # Causal mask: æ¯ä¸ª query åªèƒ½çœ‹åˆ°å®ƒä¹‹å‰çš„ keys (åŒ…æ‹¬è‡ªå·±)
        causal_mask = torch.triu(
            torch.full((query_len, total_seq_len), float("-inf"), device=q.device),
            diagonal=q_start + 1,
        )
        scores = scores + causal_mask.unsqueeze(0)
        
        # Softmax (è½¬æ¢å›åŸå§‹ dtype)
        attn_weights = F.softmax(scores, dim=-1, dtype=torch.float32).to(v.dtype)
        
        # Attention output
        # attn_weights: [num_heads, query_len, total_seq_len]
        # V: [total_seq_len, num_heads, head_dim]
        attn_output = torch.einsum("hqk,khd->qhd", attn_weights, v)
        
        return attn_output
    
    def _decode_attention(
        self,
        q: torch.Tensor,  # [1, num_heads, head_dim]
        k: torch.Tensor,  # [total_seq_len, num_kv_heads, head_dim]
        v: torch.Tensor,  # [total_seq_len, num_kv_heads, head_dim]
    ) -> torch.Tensor:
        """
        Decode é˜¶æ®µçš„ Attention è®¡ç®—
        
        ç‰¹ç‚¹:
        - æ¯æ¬¡åªå¤„ç† 1 ä¸ªæ–° token
        - æ— éœ€ causal mask (å› ä¸ºåªæœ‰ä¸€ä¸ª query)
        - Memory-boundï¼Œéœ€è¦è¯»å–å®Œæ•´ KV Cache
        """
        # GQA: æ‰©å±• K, V
        num_kv_groups = self.num_heads // self.num_kv_heads
        k = k.repeat_interleave(num_kv_groups, dim=1)
        v = v.repeat_interleave(num_kv_groups, dim=1)
        
        # è®¡ç®— attention scores
        # Q: [1, num_heads, head_dim]
        # K: [total_seq_len, num_heads, head_dim]
        scores = torch.einsum("qhd,khd->hqk", q, k) * self.scaling
        # scores: [num_heads, 1, total_seq_len]
        
        # Softmax (æ— éœ€ maskï¼Œè½¬æ¢å›åŸå§‹ dtype)
        attn_weights = F.softmax(scores, dim=-1, dtype=torch.float32).to(v.dtype)
        
        # Attention output
        attn_output = torch.einsum("hqk,khd->qhd", attn_weights, v)
        
        return attn_output


# ============================================================================
# 5. MLP å±‚
# ============================================================================

class Qwen3MLP(nn.Module):
    """
    Qwen3 çš„ MLP å±‚
    
    ç»“æ„: SiLU(gate_proj(x)) * up_proj(x) -> down_proj
    æ”¯æŒ Tensor Parallelism
    """
    
    def __init__(self, config: Qwen3Config, tp_rank: int = 0):
        super().__init__()
        
        self.tp_rank = tp_rank
        self.tp_size = config.tp_size
        
        # TP åˆ†ç‰‡: intermediate_size åœ¨ tp_size ä¸Šåˆ†ç‰‡
        self.intermediate_size_per_tp = config.intermediate_size // config.tp_size
        
        # Gate å’Œ Up æ˜¯ Column Parallel
        self.gate_proj = nn.Linear(
            config.hidden_size,
            self.intermediate_size_per_tp,
            bias=False,
        )
        self.up_proj = nn.Linear(
            config.hidden_size,
            self.intermediate_size_per_tp,
            bias=False,
        )
        
        # Down æ˜¯ Row Parallel
        self.down_proj = nn.Linear(
            self.intermediate_size_per_tp,
            config.hidden_size,
            bias=False,
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # SwiGLU activation
        gate = F.silu(self.gate_proj(x))
        up = self.up_proj(x)
        return self.down_proj(gate * up)


# ============================================================================
# 6. Decoder Layer
# ============================================================================

class Qwen3DecoderLayer(nn.Module):
    """å®Œæ•´çš„ Decoder Layer"""
    
    def __init__(self, config: Qwen3Config, layer_idx: int, tp_rank: int = 0):
        super().__init__()
        
        self.self_attn = Qwen3Attention(config, layer_idx, tp_rank)
        self.mlp = Qwen3MLP(config, tp_rank)
        
        self.input_layernorm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        positions: torch.Tensor,
        freqs_cis: torch.Tensor,
        kv_cache: Optional[PagedKVCache] = None,
        request_id: str = "req_0",
        cache_start_pos: int = 0,
        is_prefill: bool = True,
    ) -> torch.Tensor:
        # Self Attention with pre-norm
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        hidden_states = self.self_attn(
            hidden_states,
            positions,
            freqs_cis,
            kv_cache,
            request_id,
            cache_start_pos,
            is_prefill,
        )
        hidden_states = residual + hidden_states
        
        # MLP with pre-norm
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        
        return hidden_states


# ============================================================================
# 7. ä¸»æ¼”ç¤ºå‡½æ•°
# ============================================================================

def print_separator(title: str):
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)


def print_step(step_num: int, title: str):
    print(f"\n[Step {step_num}] {title}")
    print("-" * 50)


def demo_prefill_and_decode():
    """
    å®Œæ•´æ¼”ç¤º Prefill å’Œ Decode æµç¨‹
    """
    # =========== é…ç½® ===========
    # æ£€æµ‹ GPU
    if torch.cuda.is_available():
        device = "cuda"
        dtype = torch.float16  # GPU ä½¿ç”¨ fp16 æ›´å¿«
        # æ‰“å° GPU ä¿¡æ¯
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"\nğŸš€ æ£€æµ‹åˆ° GPU: {gpu_name}")
        print(f"   æ˜¾å­˜: {gpu_memory:.1f} GB")
    else:
        device = "cpu"
        dtype = torch.float32  # CPU ä½¿ç”¨ fp32
        print("\nâš ï¸  æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU è¿è¡Œ (æ€§èƒ½ä¼šè¾ƒæ…¢)")
        print("   æç¤º: ç¡®ä¿å®‰è£…äº† CUDA ç‰ˆæœ¬çš„ PyTorch:")
        print("   pip install torch --index-url https://download.pytorch.org/whl/cu121")
    
    config = Qwen3Config(tp_size=2)
    tp_rank = 0  # æ¨¡æ‹Ÿ TP rank 0
    
    print_separator("vLLM Prefill & Decode æµç¨‹æ¼”ç¤º")
    print(f"\næ¨¡å‹é…ç½® (Qwen3-4B with TP={config.tp_size}):")
    print(f"  - Hidden Size: {config.hidden_size}")
    print(f"  - Attention Heads: {config.num_attention_heads} (per GPU: {config.num_heads_per_tp})")
    print(f"  - KV Heads (GQA): {config.num_key_value_heads} (per GPU: {config.num_kv_heads_per_tp})")
    print(f"  - Head Dim: {config.head_dim}")
    print(f"  - å±‚æ•°: {config.num_hidden_layers}")
    print(f"  - Device: {device}, Dtype: {dtype}")
    
    # =========== åˆå§‹åŒ–ç»„ä»¶ ===========
    print_step(1, "åˆå§‹åŒ–ç»„ä»¶")
    
    # åªæ¼”ç¤ºä¸€å±‚ (å®é™…æ¨¡å‹æœ‰ 36 å±‚)
    layer = Qwen3DecoderLayer(config, layer_idx=0, tp_rank=tp_rank).to(device, dtype)
    
    # KV Cache
    block_size = 16
    num_blocks = 100
    kv_cache = PagedKVCache(
        num_blocks=num_blocks,
        block_size=block_size,
        num_kv_heads=config.num_kv_heads_per_tp,
        head_dim=config.head_dim,
        dtype=dtype,
        device=device,
    )
    
    # RoPE
    freqs_cis = precompute_freqs_cis(
        config.head_dim,
        config.max_position_embeddings,
        device=device,
    )
    
    print(f"  KV Cache: {num_blocks} blocks Ã— {block_size} tokens/block")
    print(f"  æ¯ä¸ª block å†…å­˜: K + V = 2 Ã— {config.num_kv_heads_per_tp} Ã— {block_size} Ã— {config.head_dim} Ã— 2 bytes")
    print(f"  æ€» KV Cache å†…å­˜: {num_blocks * 2 * config.num_kv_heads_per_tp * block_size * config.head_dim * 2 / 1024 / 1024:.2f} MB")
    
    # =========== æ¨¡æ‹Ÿè¾“å…¥ ===========
    prompt_length = 128  # prompt é•¿åº¦
    max_new_tokens = 10  # ç”Ÿæˆ token æ•°
    request_id = "req_001"
    
    # ä¸ºè¯·æ±‚åˆ†é… KV Cache blocks
    total_len = prompt_length + max_new_tokens
    kv_cache.allocate_blocks(request_id, total_len)
    print(f"\n  åˆ†é…äº† {len(kv_cache.block_tables[request_id])} ä¸ª blocks ç»™è¯·æ±‚ {request_id}")
    
    # =========== PREFILL é˜¶æ®µ ===========
    print_separator("PREFILL é˜¶æ®µ")
    print("å¤„ç†å®Œæ•´ promptï¼Œç”Ÿæˆåˆå§‹ KV Cache")
    
    # æ¨¡æ‹Ÿ embedding è¾“å‡º
    prompt_hidden = torch.randn(
        prompt_length, config.hidden_size,
        dtype=dtype, device=device
    )
    prompt_positions = torch.arange(prompt_length, device=device)
    
    print_step(2, f"Prefill: å¤„ç† {prompt_length} ä¸ª prompt tokens")
    print(f"  è¾“å…¥å½¢çŠ¶: hidden_states {list(prompt_hidden.shape)}")
    print(f"  ä½ç½®èŒƒå›´: 0 ~ {prompt_length - 1}")
    
    # GPU Warmup (ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè§¦å‘ JIT ç¼–è¯‘ï¼Œä¸è®¡å…¥æ—¶é—´)
    import time
    if device == "cuda":
        print("  [GPU Warmup ä¸­...]")
        with torch.no_grad():
            _ = layer(
                hidden_states=prompt_hidden,
                positions=prompt_positions,
                freqs_cis=freqs_cis,
                kv_cache=None,  # warmup ä¸ä½¿ç”¨ cache
                request_id="warmup",
                cache_start_pos=0,
                is_prefill=True,
            )
        torch.cuda.synchronize()
    
    # Prefill è®¡ç®—
    start = time.perf_counter()
    
    with torch.no_grad():
        prefill_output = layer(
            hidden_states=prompt_hidden,
            positions=prompt_positions,
            freqs_cis=freqs_cis,
            kv_cache=kv_cache,
            request_id=request_id,
            cache_start_pos=0,
            is_prefill=True,
        )
    
    if device == "cuda":
        torch.cuda.synchronize()
    prefill_time = time.perf_counter() - start
    
    print(f"  è¾“å‡ºå½¢çŠ¶: {list(prefill_output.shape)}")
    print(f"  Prefill æ—¶é—´: {prefill_time * 1000:.2f} ms")
    print(f"  ååé‡: {prompt_length / prefill_time:.0f} tokens/s")
    
    # è®¡ç®— FLOPs
    # QKV + O projections
    qkvo_flops = 2 * prompt_length * config.hidden_size * (
        config.num_heads_per_tp * config.head_dim +  # Q
        2 * config.num_kv_heads_per_tp * config.head_dim +  # K, V
        config.num_heads_per_tp * config.head_dim  # O
    )
    # Attention
    attn_flops = 2 * prompt_length * prompt_length * config.num_heads_per_tp * config.head_dim
    # MLP
    mlp_flops = 2 * prompt_length * config.hidden_size * 3 * (config.intermediate_size // config.tp_size)
    
    total_flops = qkvo_flops + attn_flops + mlp_flops
    print(f"  å•å±‚ FLOPs: {total_flops / 1e9:.2f} GFLOPs")
    
    # =========== Chunked Prefill æ¼”ç¤º ===========
    print_separator("CHUNKED PREFILL æ¼”ç¤º")
    print("å°†é•¿ prompt åˆ†æˆå¤šä¸ª chunk å¤„ç†")
    
    long_prompt_length = 512
    chunk_size = 128
    num_chunks = math.ceil(long_prompt_length / chunk_size)
    
    # é‡æ–°åˆå§‹åŒ–è¯·æ±‚ - ä¸ºé•¿ prompt åˆ†é…è¶³å¤Ÿçš„ blocks
    kv_cache.free(request_id)
    kv_cache.allocate_blocks(request_id, long_prompt_length + max_new_tokens)
    
    print(f"\n  é•¿ Prompt: {long_prompt_length} tokens")
    print(f"  Chunk å¤§å°: {chunk_size} tokens")
    print(f"  Chunk æ•°é‡: {num_chunks}")
    
    long_prompt_hidden = torch.randn(
        long_prompt_length, config.hidden_size,
        dtype=dtype, device=device
    )
    
    total_chunked_time = 0
    for chunk_idx in range(num_chunks):
        start_pos = chunk_idx * chunk_size
        end_pos = min(start_pos + chunk_size, long_prompt_length)
        chunk_len = end_pos - start_pos
        
        chunk_hidden = long_prompt_hidden[start_pos:end_pos]
        chunk_positions = torch.arange(start_pos, end_pos, device=device)
        
        print_step(3 + chunk_idx, f"Chunk {chunk_idx + 1}/{num_chunks}: tokens [{start_pos}, {end_pos})")
        
        start = time.perf_counter()
        with torch.no_grad():
            chunk_output = layer(
                hidden_states=chunk_hidden,
                positions=chunk_positions,
                freqs_cis=freqs_cis,
                kv_cache=kv_cache,
                request_id=request_id,
                cache_start_pos=start_pos,
                is_prefill=True,  # ä»ç„¶æ˜¯ prefill æ¨¡å¼
            )
        if device == "cuda":
            torch.cuda.synchronize()
        chunk_time = time.perf_counter() - start
        total_chunked_time += chunk_time
        
        print(f"    å¤„ç† {chunk_len} tokensï¼Œè€—æ—¶ {chunk_time * 1000:.2f} ms")
        print(f"    ç´¯è®¡å·²ç¼“å­˜ KV: {end_pos} tokens")
    
    print(f"\n  Chunked Prefill æ€»æ—¶é—´: {total_chunked_time * 1000:.2f} ms")
    
    # =========== DECODE é˜¶æ®µ ===========
    print_separator("DECODE é˜¶æ®µ")
    print("é€ token è‡ªå›å½’ç”Ÿæˆ")
    
    # é‡ç½®ä¸ºçŸ­ prompt
    kv_cache.free(request_id)
    kv_cache.allocate_blocks(request_id, prompt_length + max_new_tokens)
    
    # å…ˆåšä¸€æ¬¡ prefill
    with torch.no_grad():
        _ = layer(
            hidden_states=prompt_hidden,
            positions=prompt_positions,
            freqs_cis=freqs_cis,
            kv_cache=kv_cache,
            request_id=request_id,
            cache_start_pos=0,
            is_prefill=True,
        )
    
    current_pos = prompt_length
    decode_times = []
    
    for step in range(max_new_tokens):
        print_step(3 + num_chunks + step, f"Decode step {step + 1}/{max_new_tokens}")
        
        # æ¨¡æ‹Ÿæ–°ç”Ÿæˆçš„ token çš„ embedding
        new_token_hidden = torch.randn(
            1, config.hidden_size,
            dtype=dtype, device=device
        )
        new_position = torch.tensor([current_pos], device=device)
        
        start = time.perf_counter()
        with torch.no_grad():
            decode_output = layer(
                hidden_states=new_token_hidden,
                positions=new_position,
                freqs_cis=freqs_cis,
                kv_cache=kv_cache,
                request_id=request_id,
                cache_start_pos=current_pos,
                is_prefill=False,  # Decode æ¨¡å¼
            )
        if device == "cuda":
            torch.cuda.synchronize()
        decode_time = time.perf_counter() - start
        decode_times.append(decode_time * 1000)
        
        print(f"    ä½ç½®: {current_pos}")
        print(f"    éœ€è¦è¯»å– KV Cache: {current_pos + 1} tokens")
        print(f"    è€—æ—¶: {decode_time * 1000:.2f} ms")
        
        current_pos += 1
    
    # =========== ç»Ÿè®¡ä¿¡æ¯ ===========
    print_separator("ç»Ÿè®¡ä¿¡æ¯")
    
    avg_decode_time = sum(decode_times) / len(decode_times)
    print(f"\n  Prefill ({prompt_length} tokens):")
    print(f"    æ—¶é—´: {prefill_time * 1000:.2f} ms")
    print(f"    ååé‡: {prompt_length / prefill_time:.0f} tokens/s")
    print(f"    ç‰¹ç‚¹: Compute-bound (çŸ©é˜µä¹˜æ³•å¯†é›†)")
    
    print(f"\n  Decode ({max_new_tokens} tokens):")
    print(f"    å¹³å‡æ¯ token: {avg_decode_time:.2f} ms")
    print(f"    ååé‡: {1000 / avg_decode_time:.0f} tokens/s")
    print(f"    ç‰¹ç‚¹: Memory-bound (KV Cache è¯»å–)")
    
    # KV Cache å†…å­˜è®¡ç®—
    kv_cache_bytes_per_token = 2 * config.num_kv_heads_per_tp * config.head_dim * 2  # K + V, bf16
    print(f"\n  KV Cache å†…å­˜ (per token, per layer, per GPU):")
    print(f"    {kv_cache_bytes_per_token} bytes = 2 (K+V) Ã— {config.num_kv_heads_per_tp} heads Ã— {config.head_dim} dim Ã— 2 bytes")
    print(f"    å…¨æ¨¡å‹ ({config.num_hidden_layers} å±‚): {kv_cache_bytes_per_token * config.num_hidden_layers / 1024:.1f} KB/token")
    
    # è®¡ç®—å¯¹æ¯”
    print(f"\n  Prefill vs Decode ç‰¹æ€§å¯¹æ¯”:")
    print(f"    {'ç‰¹æ€§':<20} {'Prefill':<25} {'Decode':<25}")
    print(f"    {'-'*70}")
    print(f"    {'Query tokens':<20} {prompt_length:<25} {'1':<25}")
    print(f"    {'KV tokens':<20} {prompt_length:<25} {f'1~{prompt_length + max_new_tokens}':<25}")
    print(f"    {'Attention å¤æ‚åº¦':<20} {'O(seq_lenÂ²)':<25} {'O(seq_len)':<25}")
    print(f"    {'ç“¶é¢ˆ':<20} {'Compute':<25} {'Memory Bandwidth':<25}")
    print(f"    {'Batching æ”¶ç›Š':<20} {'é«˜ (æ›´å¤šå¹¶è¡Œ)':<25} {'ä¸­ç­‰ (å—é™äºå†…å­˜)':<25}")
    
    # GPU å†…å­˜ä½¿ç”¨æƒ…å†µ
    if device == "cuda":
        print(f"\n  GPU å†…å­˜ä½¿ç”¨:")
        allocated = torch.cuda.memory_allocated() / 1024**2
        reserved = torch.cuda.memory_reserved() / 1024**2
        print(f"    å·²åˆ†é…: {allocated:.1f} MB")
        print(f"    å·²ä¿ç•™: {reserved:.1f} MB")
    
    print("\n" + "=" * 70)
    print("  æ¼”ç¤ºå®Œæˆ!")
    print("=" * 70)


# ============================================================================
# 8. è¿è¡Œæ¼”ç¤º
# ============================================================================

if __name__ == "__main__":
    demo_prefill_and_decode()

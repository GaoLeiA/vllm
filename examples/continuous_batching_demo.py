#!/usr/bin/env python3
"""
Continuous Batching æ¼”ç¤º

å±•ç¤º vLLM çš„æ ¸å¿ƒè°ƒåº¦ç­–ç•¥ï¼š
1. è¯·æ±‚åŠ¨æ€åŠ å…¥å’Œé€€å‡º batch
2. GPU åˆ©ç”¨ç‡æœ€å¤§åŒ–
3. Prefill å’Œ Decode æ··åˆè°ƒåº¦
"""

import time
import random
from dataclasses import dataclass, field
from typing import Optional
from collections import deque

# ============================================================================
# 1. è¯·æ±‚å’Œè°ƒåº¦å™¨æ•°æ®ç»“æ„
# ============================================================================

@dataclass
class Request:
    """æ¨¡æ‹Ÿä¸€ä¸ªæ¨ç†è¯·æ±‚"""
    request_id: str
    prompt_tokens: int          # prompt é•¿åº¦
    max_new_tokens: int         # æœ€å¤§ç”Ÿæˆé•¿åº¦
    
    # çŠ¶æ€
    status: str = "waiting"     # waiting, prefilling, decoding, finished
    computed_tokens: int = 0    # å·²è®¡ç®—çš„ token æ•°
    generated_tokens: int = 0   # å·²ç”Ÿæˆçš„ token æ•°
    
    # æ—¶é—´ç»Ÿè®¡
    arrival_time: float = field(default_factory=time.time)
    first_token_time: Optional[float] = None
    finish_time: Optional[float] = None
    
    @property
    def total_tokens(self) -> int:
        return self.prompt_tokens + self.generated_tokens
    
    @property
    def remaining_tokens(self) -> int:
        return self.max_new_tokens - self.generated_tokens
    
    @property
    def is_prefilling(self) -> bool:
        return self.computed_tokens < self.prompt_tokens
    
    @property
    def ttft(self) -> Optional[float]:
        """Time To First Token"""
        if self.first_token_time:
            return self.first_token_time - self.arrival_time
        return None
    
    @property
    def total_time(self) -> Optional[float]:
        if self.finish_time:
            return self.finish_time - self.arrival_time
        return None


@dataclass 
class SchedulerConfig:
    """è°ƒåº¦å™¨é…ç½®"""
    max_num_seqs: int = 8               # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    max_num_batched_tokens: int = 2048  # æ¯æ­¥æœ€å¤§ token æ•°
    enable_chunked_prefill: bool = True # å¯ç”¨åˆ†å— prefill
    chunk_size: int = 512               # Prefill chunk å¤§å°


class ContinuousBatchingScheduler:
    """
    Continuous Batching è°ƒåº¦å™¨
    
    æ ¸å¿ƒåŠŸèƒ½:
    1. åŠ¨æ€ç®¡ç† running batch
    2. è¯·æ±‚å®Œæˆåç«‹å³é‡Šæ”¾èµ„æº
    3. æ–°è¯·æ±‚å¯ä»¥éšæ—¶åŠ å…¥
    4. Prefill å’Œ Decode æ··åˆè°ƒåº¦
    """
    
    def __init__(self, config: SchedulerConfig):
        self.config = config
        
        # è¯·æ±‚é˜Ÿåˆ—
        self.waiting_queue: deque[Request] = deque()  # ç­‰å¾…é˜Ÿåˆ—
        self.running_batch: list[Request] = []        # å½“å‰è¿è¡Œçš„ batch
        self.finished_requests: list[Request] = []   # å·²å®Œæˆçš„è¯·æ±‚
        
        # ç»Ÿè®¡
        self.step_count = 0
        self.total_tokens_processed = 0
    
    def add_request(self, request: Request):
        """æ·»åŠ æ–°è¯·æ±‚åˆ°ç­‰å¾…é˜Ÿåˆ—"""
        request.status = "waiting"
        self.waiting_queue.append(request)
        print(f"  ğŸ“¥ è¯·æ±‚ {request.request_id} åŠ å…¥ç­‰å¾…é˜Ÿåˆ— "
              f"(prompt={request.prompt_tokens}, max_new={request.max_new_tokens})")
    
    def schedule(self) -> dict:
        """
        è°ƒåº¦ä¸‹ä¸€æ­¥è¦æ‰§è¡Œçš„è¯·æ±‚
        
        è¿”å›:
            scheduled_tokens: dict[request_id -> num_tokens]
        """
        scheduled_tokens: dict[str, int] = {}
        token_budget = self.config.max_num_batched_tokens
        
        # ========== Step 1: è°ƒåº¦å·²åœ¨è¿è¡Œçš„è¯·æ±‚ (Decode) ==========
        for req in self.running_batch[:]:
            if req.is_prefilling:
                # Chunked Prefill: ç»§ç»­ prefill
                remaining_prefill = req.prompt_tokens - req.computed_tokens
                if self.config.enable_chunked_prefill:
                    tokens_to_schedule = min(remaining_prefill, 
                                            self.config.chunk_size,
                                            token_budget)
                else:
                    tokens_to_schedule = min(remaining_prefill, token_budget)
            else:
                # Decode: æ¯æ¬¡ 1 ä¸ª token
                tokens_to_schedule = 1
            
            if tokens_to_schedule > 0 and token_budget >= tokens_to_schedule:
                scheduled_tokens[req.request_id] = tokens_to_schedule
                token_budget -= tokens_to_schedule
        
        # ========== Step 2: ä»ç­‰å¾…é˜Ÿåˆ—è°ƒåº¦æ–°è¯·æ±‚ (Prefill) ==========
        while (self.waiting_queue and 
               len(self.running_batch) < self.config.max_num_seqs and
               token_budget > 0):
            
            req = self.waiting_queue[0]
            
            # è®¡ç®—éœ€è¦è°ƒåº¦çš„ prefill tokens
            if self.config.enable_chunked_prefill:
                tokens_to_schedule = min(req.prompt_tokens,
                                        self.config.chunk_size,
                                        token_budget)
            else:
                # ä¸å¯ç”¨ chunked prefillï¼Œå¿…é¡»ä¸€æ¬¡æ€§å¤„ç†å®Œæ•´ prompt
                if req.prompt_tokens > token_budget:
                    break
                tokens_to_schedule = req.prompt_tokens
            
            if tokens_to_schedule > 0:
                self.waiting_queue.popleft()
                req.status = "prefilling"
                self.running_batch.append(req)
                scheduled_tokens[req.request_id] = tokens_to_schedule
                token_budget -= tokens_to_schedule
        
        return scheduled_tokens
    
    def update(self, scheduled_tokens: dict[str, int]):
        """
        æ›´æ–°è¯·æ±‚çŠ¶æ€ (æ¨¡æ‹Ÿæ‰§è¡Œå®Œæˆ)
        """
        current_time = time.time()
        finished_this_step = []
        
        for req in self.running_batch:
            if req.request_id not in scheduled_tokens:
                continue
            
            num_tokens = scheduled_tokens[req.request_id]
            req.computed_tokens += num_tokens
            
            # æ£€æŸ¥æ˜¯å¦å®Œæˆ prefill
            if req.is_prefilling:
                if req.computed_tokens >= req.prompt_tokens:
                    req.status = "decoding"
            else:
                # Decode é˜¶æ®µ: ç”Ÿæˆæ–° token
                req.generated_tokens += 1
                
                # è®°å½•é¦– token æ—¶é—´
                if req.first_token_time is None:
                    req.first_token_time = current_time
                
                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if req.generated_tokens >= req.max_new_tokens:
                    req.status = "finished"
                    req.finish_time = current_time
                    finished_this_step.append(req)
        
        # ç§»é™¤å·²å®Œæˆçš„è¯·æ±‚
        for req in finished_this_step:
            self.running_batch.remove(req)
            self.finished_requests.append(req)
        
        self.step_count += 1
        self.total_tokens_processed += sum(scheduled_tokens.values())
        
        return finished_this_step
    
    def get_batch_info(self) -> dict:
        """è·å–å½“å‰ batch ä¿¡æ¯"""
        prefilling = [r for r in self.running_batch if r.is_prefilling]
        decoding = [r for r in self.running_batch if not r.is_prefilling]
        
        return {
            "step": self.step_count,
            "running": len(self.running_batch),
            "waiting": len(self.waiting_queue),
            "finished": len(self.finished_requests),
            "prefilling": len(prefilling),
            "decoding": len(decoding),
        }


# ============================================================================
# 2. å¯è§†åŒ–è¾“å‡º
# ============================================================================

def print_batch_state(scheduler: ContinuousBatchingScheduler, 
                      scheduled_tokens: dict[str, int],
                      finished: list[Request]):
    """æ‰“å°å½“å‰ batch çŠ¶æ€"""
    info = scheduler.get_batch_info()
    
    # æ„å»º batch å¯è§†åŒ–
    batch_viz = []
    for req in scheduler.running_batch:
        tokens = scheduled_tokens.get(req.request_id, 0)
        if req.is_prefilling:
            progress = req.computed_tokens / req.prompt_tokens
            batch_viz.append(f"{req.request_id}[P{progress*100:.0f}%:{tokens}t]")
        else:
            progress = req.generated_tokens / req.max_new_tokens
            batch_viz.append(f"{req.request_id}[D{progress*100:.0f}%:{tokens}t]")
    
    # æ‰“å°çŠ¶æ€
    print(f"\n{'â”€'*70}")
    print(f"Step {info['step']:3d} â”‚ Running: {info['running']}/{scheduler.config.max_num_seqs} â”‚ "
          f"Waiting: {info['waiting']} â”‚ Finished: {info['finished']} â”‚ "
          f"Prefill: {info['prefilling']} â”‚ Decode: {info['decoding']}")
    print(f"        â”‚ Batch: [{', '.join(batch_viz) if batch_viz else 'empty'}]")
    
    if finished:
        for req in finished:
            print(f"        â”‚ âœ… {req.request_id} å®Œæˆ! "
                  f"TTFT={req.ttft*1000:.1f}ms, Total={req.total_time*1000:.1f}ms, "
                  f"Tokens={req.generated_tokens}")


def print_separator(title: str):
    print(f"\n{'='*70}")
    print(f"  {title}")
    print('='*70)


# ============================================================================
# 3. æ¼”ç¤ºåœºæ™¯
# ============================================================================

def demo_continuous_batching():
    """
    æ¼”ç¤º Continuous Batching çš„å·¥ä½œæµç¨‹
    """
    print_separator("Continuous Batching æ¼”ç¤º")
    
    # é…ç½®
    config = SchedulerConfig(
        max_num_seqs=4,              # æœ€å¤šåŒæ—¶å¤„ç† 4 ä¸ªè¯·æ±‚
        max_num_batched_tokens=1024, # æ¯æ­¥æœ€å¤š 1024 tokens
        enable_chunked_prefill=True,
        chunk_size=256,              # Prefill åˆ†å—å¤§å°
    )
    
    scheduler = ContinuousBatchingScheduler(config)
    
    print(f"\nè°ƒåº¦å™¨é…ç½®:")
    print(f"  - æœ€å¤§å¹¶å‘è¯·æ±‚: {config.max_num_seqs}")
    print(f"  - æ¯æ­¥æœ€å¤§ tokens: {config.max_num_batched_tokens}")
    print(f"  - Chunked Prefill: {'å¯ç”¨' if config.enable_chunked_prefill else 'ç¦ç”¨'}")
    print(f"  - Chunk å¤§å°: {config.chunk_size}")
    
    # ========== åœºæ™¯ 1: åˆå§‹è¯·æ±‚ ==========
    print_separator("åœºæ™¯ 1: åˆå§‹è¯·æ±‚åˆ°è¾¾")
    
    initial_requests = [
        Request("A", prompt_tokens=512, max_new_tokens=100),
        Request("B", prompt_tokens=256, max_new_tokens=50),
        Request("C", prompt_tokens=128, max_new_tokens=30),
    ]
    
    for req in initial_requests:
        scheduler.add_request(req)
    
    # è¿è¡Œå‡ æ­¥
    for _ in range(8):
        scheduled = scheduler.schedule()
        finished = scheduler.update(scheduled)
        print_batch_state(scheduler, scheduled, finished)
        time.sleep(0.05)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
    
    # ========== åœºæ™¯ 2: æ–°è¯·æ±‚åŠ¨æ€åŠ å…¥ ==========
    print_separator("åœºæ™¯ 2: æ–°è¯·æ±‚åŠ¨æ€åŠ å…¥")
    print("  (æ¨¡æ‹ŸçœŸå®åœºæ™¯: è¯·æ±‚ B å®Œæˆåï¼Œæ–°è¯·æ±‚ D ç«‹å³åŠ å…¥)")
    
    new_request = Request("D", prompt_tokens=200, max_new_tokens=40)
    scheduler.add_request(new_request)
    
    for _ in range(10):
        # éšæœºæ·»åŠ æ–°è¯·æ±‚ (æ¨¡æ‹ŸçœŸå®æµé‡)
        if random.random() < 0.2 and len(scheduler.waiting_queue) < 3:
            req_id = chr(ord('E') + len(scheduler.finished_requests) + 
                        len(scheduler.running_batch) + len(scheduler.waiting_queue) - 3)
            new_req = Request(
                req_id, 
                prompt_tokens=random.randint(64, 256),
                max_new_tokens=random.randint(20, 60)
            )
            scheduler.add_request(new_req)
        
        scheduled = scheduler.schedule()
        if not scheduled:
            break
        finished = scheduler.update(scheduled)
        print_batch_state(scheduler, scheduled, finished)
        time.sleep(0.05)
    
    # ========== åœºæ™¯ 3: å¤„ç†å®Œæ‰€æœ‰è¯·æ±‚ ==========
    print_separator("åœºæ™¯ 3: å¤„ç†å‰©ä½™è¯·æ±‚")
    
    while scheduler.running_batch or scheduler.waiting_queue:
        scheduled = scheduler.schedule()
        if not scheduled:
            break
        finished = scheduler.update(scheduled)
        print_batch_state(scheduler, scheduled, finished)
        time.sleep(0.02)
    
    # ========== ç»Ÿè®¡ä¿¡æ¯ ==========
    print_separator("ç»Ÿè®¡ä¿¡æ¯")
    
    print(f"\n  æ€»æ­¥æ•°: {scheduler.step_count}")
    print(f"  æ€»å¤„ç† tokens: {scheduler.total_tokens_processed}")
    print(f"  å®Œæˆè¯·æ±‚æ•°: {len(scheduler.finished_requests)}")
    
    if scheduler.finished_requests:
        ttfts = [r.ttft for r in scheduler.finished_requests if r.ttft]
        total_times = [r.total_time for r in scheduler.finished_requests if r.total_time]
        
        print(f"\n  å¹³å‡ TTFT: {sum(ttfts)/len(ttfts)*1000:.1f} ms")
        print(f"  å¹³å‡æ€»æ—¶é—´: {sum(total_times)/len(total_times)*1000:.1f} ms")
        
        print(f"\n  å„è¯·æ±‚è¯¦æƒ…:")
        for req in scheduler.finished_requests:
            print(f"    {req.request_id}: prompt={req.prompt_tokens}, "
                  f"generated={req.generated_tokens}, "
                  f"TTFT={req.ttft*1000:.1f}ms, "
                  f"Total={req.total_time*1000:.1f}ms")


def demo_comparison():
    """
    å¯¹æ¯” Static Batching vs Continuous Batching
    """
    print_separator("Static vs Continuous Batching å¯¹æ¯”")
    
    requests = [
        ("A", 100, 200),  # é•¿è¯·æ±‚
        ("B", 100, 50),   # çŸ­è¯·æ±‚
        ("C", 100, 80),   # ä¸­ç­‰è¯·æ±‚
    ]
    
    # ========== Static Batching æ¨¡æ‹Ÿ ==========
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Static Batching â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ æ‰€æœ‰è¯·æ±‚å¿…é¡»ç­‰å¾…æœ€é•¿çš„è¯·æ±‚å®Œæˆ                   â”‚")
    
    max_tokens = max(t[2] for t in requests)
    total_steps = 100 + max_tokens  # prefill + max decode
    
    print(f"â”‚                                               â”‚")
    print(f"â”‚ æ€»æ­¥æ•°: {total_steps} (å—é™äºæœ€é•¿è¯·æ±‚ A)          â”‚")
    print(f"â”‚                                               â”‚")
    print(f"â”‚ æ—¶é—´çº¿:                                       â”‚")
    print(f"â”‚ Step 1-100:   [A, B, C] prefill               â”‚")
    print(f"â”‚ Step 101-150: [A, -, C] Bå®Œæˆ,GPUç©ºé—²50%        â”‚")
    print(f"â”‚ Step 151-180: [A, -, -] Cå®Œæˆ,GPUç©ºé—²67%        â”‚")
    print(f"â”‚ Step 181-300: [A, -, -] åªæœ‰Aåœ¨è¿è¡Œ,GPUç©ºé—²67%  â”‚")
    print(f"â”‚                                               â”‚")
    print(f"â”‚ GPU åˆ©ç”¨ç‡: ~45%                              â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # ========== Continuous Batching æ¨¡æ‹Ÿ ==========
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Continuous Batching â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ å®Œæˆçš„è¯·æ±‚ç«‹å³é‡Šæ”¾,æ–°è¯·æ±‚éšæ—¶åŠ å…¥               â”‚")
    print(f"â”‚                                               â”‚")
    print(f"â”‚ æ—¶é—´çº¿:                                       â”‚")
    print(f"â”‚ Step 1-100:   [A, B, C] prefill               â”‚")
    print(f"â”‚ Step 101-150: [A, D, C] Bå®Œæˆ,Dç«‹å³åŠ å…¥        â”‚")
    print(f"â”‚ Step 151-180: [A, D, E] Cå®Œæˆ,Eç«‹å³åŠ å…¥        â”‚")
    print(f"â”‚ Step 181-200: [A, F, E] Då®Œæˆ,Fç«‹å³åŠ å…¥        â”‚")
    print(f"â”‚ ...          [æŒç»­å¤„ç†æ–°è¯·æ±‚]                 â”‚")
    print(f"â”‚                                               â”‚")
    print(f"â”‚ GPU åˆ©ç”¨ç‡: ~95%+                             â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # æ•°å€¼å¯¹æ¯”
    print("\næ€§èƒ½å¯¹æ¯”:")
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ æŒ‡æ ‡            â”‚ Static        â”‚ Continuous        â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚ GPU åˆ©ç”¨ç‡      â”‚ ~45%          â”‚ ~95%+             â”‚")
    print("â”‚ è¯·æ±‚ B ç­‰å¾…æ—¶é—´ â”‚ 300 steps     â”‚ 150 steps         â”‚")
    print("â”‚ ååé‡          â”‚ 1x            â”‚ 2-3x              â”‚")
    print("â”‚ å†…å­˜æ•ˆç‡        â”‚ ä½ (é¢„åˆ†é…)   â”‚ é«˜ (åŠ¨æ€åˆ†é…)     â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")


# ============================================================================
# 4. è¿è¡Œæ¼”ç¤º
# ============================================================================

if __name__ == "__main__":
    demo_continuous_batching()
    print("\n")
    demo_comparison()
    
    print("\n" + "="*70)
    print("  æ¼”ç¤ºå®Œæˆ!")
    print("="*70)
